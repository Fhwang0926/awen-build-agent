// LLMService.js - LLM API í†µí•© ì„œë¹„ìŠ¤

const https = require('https');

// í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’: OpenAI)
const OPENAI_API_KEY = process.env.OPENAI_API_KEY || process.env.LLM_API_KEY;
const LLM_PROVIDER = process.env.LLM_PROVIDER || 'openai'; // 'openai' ë˜ëŠ” 'anthropic'
const ANTHROPIC_API_KEY = process.env.ANTHROPIC_API_KEY;
// ëª¨ë¸ ì„¤ì • (ê¸°ë³¸ê°’: nano - ê°€ì¥ ì‘ì€ ëª¨ë¸)
const OPENAI_MODEL = process.env.OPENAI_MODEL || 'gpt-5-nano'; // nano ëª¨ë¸
const ANTHROPIC_MODEL = process.env.ANTHROPIC_MODEL || 'claude-3-haiku-20240307';

/**
 * LLM API í˜¸ì¶œ í•¨ìˆ˜
 * @param {string} prompt - LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸
 * @param {string} systemPrompt - ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒì )
 * @param {string} context - í˜¸ì¶œ ì»¨í…ìŠ¤íŠ¸ (ì—ì´ì „íŠ¸ ì´ë¦„ ë“±, ì„ íƒì )
 * @returns {Promise<{response: string, usage: object}>} - LLM ì‘ë‹µê³¼ í† í° ì‚¬ìš©ëŸ‰
 */
async function callLLM(prompt, systemPrompt = null, context = 'LLM', timeout = 60000) {
    if (!OPENAI_API_KEY && !ANTHROPIC_API_KEY) {
        throw new Error('LLM API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. OPENAI_API_KEY ë˜ëŠ” ANTHROPIC_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.');
    }

    // ì…ë ¥ ë°ì´í„° í¬ê¸° ê³„ì‚° (ëŒ€ëµì )
    const inputData = {
        prompt: prompt.length,
        systemPrompt: systemPrompt ? systemPrompt.length : 0,
        totalChars: prompt.length + (systemPrompt ? systemPrompt.length : 0)
    };

    // íƒ€ì„ì•„ì›ƒ ë˜í¼
    const timeoutPromise = new Promise((_, reject) => {
        setTimeout(() => reject(new Error(`LLM í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ (${timeout}ms)`)), timeout);
    });

    try {
        const apiPromise = LLM_PROVIDER === 'anthropic' && ANTHROPIC_API_KEY
            ? callAnthropicAPI(prompt, systemPrompt, context, inputData)
            : callOpenAIAPI(prompt, systemPrompt, context, inputData);
        
        const result = await Promise.race([apiPromise, timeoutPromise]);
        return result;
    } catch (error) {
        console.error(`   âš ï¸ LLM í˜¸ì¶œ ì‹¤íŒ¨ (${context}): ${error.message}`);
        throw error;
    }
}

/**
 * OpenAI API í˜¸ì¶œ
 */
async function callOpenAIAPI(prompt, systemPrompt, context, inputData) {
    return new Promise((resolve, reject) => {
        const messages = [];
        if (systemPrompt) {
            messages.push({ role: 'system', content: systemPrompt });
        }
        messages.push({ role: 'user', content: prompt });

        // ëª¨ë¸ì— ë”°ë¼ ì˜¬ë°”ë¥¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©
        // ìµœì‹  OpenAI ëª¨ë¸ë“¤ì€ max_completion_tokensë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
        const requestBody = {
            model: OPENAI_MODEL, // ê¸°ë³¸ê°’: gpt-3.5-turbo (nano)
            messages: messages,
            temperature: 1
        };
        
        // ê¸°ë³¸ì ìœ¼ë¡œ max_completion_tokens ì‚¬ìš© (ìµœì‹  ëª¨ë¸ ëŒ€ë¶€ë¶„ ì§€ì›)
        // êµ¬í˜• ëª¨ë¸ë§Œ max_tokens ì‚¬ìš©
        const modelLower = OPENAI_MODEL.toLowerCase();
        const isLegacyModel = modelLower.startsWith('gpt-4') && 
                             !modelLower.includes('gpt-4o') && 
                             !modelLower.includes('gpt-4-turbo') &&
                             !modelLower.includes('gpt-4o-mini');
        
        if (isLegacyModel) {
            // êµ¬í˜• ëª¨ë¸ë§Œ max_tokens ì‚¬ìš©
            requestBody.max_tokens = 2000;
        } else {
            // ìµœì‹  ëª¨ë¸ì€ max_completion_tokens ì‚¬ìš© (ê¸°ë³¸ê°’)
            requestBody.max_completion_tokens = 2000;
        }
        
        const data = JSON.stringify(requestBody);

        const dataBuffer = Buffer.from(data, 'utf8');

        const options = {
            hostname: 'api.openai.com',
            path: '/v1/chat/completions',
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${OPENAI_API_KEY}`,
                'Content-Length': dataBuffer.length
            }
        };

        const req = https.request(options, (res) => {
            let responseData = '';

            res.on('data', (chunk) => {
                responseData += chunk;
            });

            res.on('end', () => {
                try {
                    // HTTP ì—ëŸ¬ ìƒíƒœ ì½”ë“œ í™•ì¸
                    if (res.statusCode !== 200) {
                        reject(new Error(`OpenAI API HTTP ì˜¤ë¥˜ (${res.statusCode}): ${responseData}`));
                        return;
                    }

                    const jsonResponse = JSON.parse(responseData);
                    if (jsonResponse.error) {
                        reject(new Error(`OpenAI API ì˜¤ë¥˜: ${jsonResponse.error.message}`));
                    } else if (jsonResponse.choices && jsonResponse.choices[0] && jsonResponse.choices[0].message) {
                        const responseText = jsonResponse.choices[0].message.content;
                        const usage = jsonResponse.usage || {};
                        
                        // í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½ ì¶œë ¥
                        console.log(`\nğŸ“Š [${context}] LLM í† í° ì‚¬ìš©ëŸ‰:`);
                        console.log(`   ëª¨ë¸: ${OPENAI_MODEL}`);
                        console.log(`   ì…ë ¥ ë°ì´í„°: ${inputData.totalChars}ì (í”„ë¡¬í”„íŠ¸: ${inputData.prompt}ì, ì‹œìŠ¤í…œ: ${inputData.systemPrompt}ì)`);
                        console.log(`   ìš”ì²­ í† í°: ${usage.prompt_tokens || 'N/A'}`);
                        console.log(`   ì‘ë‹µ í† í°: ${usage.completion_tokens || 'N/A'}`);
                        console.log(`   ì´ í† í°: ${usage.total_tokens || 'N/A'}`);
                        console.log(`   ì‘ë‹µ ê¸¸ì´: ${responseText.length}ì`);
                        
                        resolve({
                            response: responseText,
                            usage: {
                                prompt_tokens: usage.prompt_tokens || 0,
                                completion_tokens: usage.completion_tokens || 0,
                                total_tokens: usage.total_tokens || 0,
                                input_chars: inputData.totalChars,
                                response_chars: responseText.length
                            }
                        });
                    } else {
                        reject(new Error(`ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ í˜•ì‹: ${JSON.stringify(jsonResponse)}`));
                    }
                } catch (error) {
                    reject(new Error(`ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: ${error.message}, ì‘ë‹µ: ${responseData.substring(0, 500)}`));
                }
            });
        });

        req.on('error', (error) => {
            reject(new Error(`OpenAI API ìš”ì²­ ì‹¤íŒ¨: ${error.message}`));
        });

        req.write(dataBuffer);
        req.end();
    });
}

/**
 * Anthropic Claude API í˜¸ì¶œ
 */
async function callAnthropicAPI(prompt, systemPrompt, context, inputData) {
    return new Promise((resolve, reject) => {
        const messages = [{ role: 'user', content: prompt }];
        const body = {
            model: ANTHROPIC_MODEL, // ê¸°ë³¸ê°’: claude-3-haiku (ê°€ì¥ ì‘ì€ ëª¨ë¸)
            max_tokens: 2000,
            messages: messages
        };

        if (systemPrompt) {
            body.system = systemPrompt;
        }

        const data = JSON.stringify(body);
        const dataBuffer = Buffer.from(data, 'utf8');

        const options = {
            hostname: 'api.anthropic.com',
            path: '/v1/messages',
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'x-api-key': ANTHROPIC_API_KEY,
                'anthropic-version': '2023-06-01',
                'Content-Length': dataBuffer.length
            }
        };

        const req = https.request(options, (res) => {
            let responseData = '';

            res.on('data', (chunk) => {
                responseData += chunk;
            });

            res.on('end', () => {
                try {
                    // HTTP ì—ëŸ¬ ìƒíƒœ ì½”ë“œ í™•ì¸
                    if (res.statusCode !== 200) {
                        reject(new Error(`Anthropic API HTTP ì˜¤ë¥˜ (${res.statusCode}): ${responseData}`));
                        return;
                    }

                    const jsonResponse = JSON.parse(responseData);
                    if (jsonResponse.error) {
                        reject(new Error(`Anthropic API ì˜¤ë¥˜: ${jsonResponse.error.message}`));
                    } else if (jsonResponse.content && jsonResponse.content[0] && jsonResponse.content[0].text) {
                        const responseText = jsonResponse.content[0].text;
                        const usage = jsonResponse.usage || {};
                        
                        // í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½ ì¶œë ¥
                        console.log(`\nğŸ“Š [${context}] LLM í† í° ì‚¬ìš©ëŸ‰:`);
                        console.log(`   ëª¨ë¸: ${ANTHROPIC_MODEL}`);
                        console.log(`   ì…ë ¥ ë°ì´í„°: ${inputData.totalChars}ì (í”„ë¡¬í”„íŠ¸: ${inputData.prompt}ì, ì‹œìŠ¤í…œ: ${inputData.systemPrompt}ì)`);
                        console.log(`   ìš”ì²­ í† í°: ${usage.input_tokens || 'N/A'}`);
                        console.log(`   ì‘ë‹µ í† í°: ${usage.output_tokens || 'N/A'}`);
                        console.log(`   ì´ í† í°: ${(usage.input_tokens || 0) + (usage.output_tokens || 0)}`);
                        console.log(`   ì‘ë‹µ ê¸¸ì´: ${responseText.length}ì`);
                        
                        resolve({
                            response: responseText,
                            usage: {
                                prompt_tokens: usage.input_tokens || 0,
                                completion_tokens: usage.output_tokens || 0,
                                total_tokens: (usage.input_tokens || 0) + (usage.output_tokens || 0),
                                input_chars: inputData.totalChars,
                                response_chars: responseText.length
                            }
                        });
                    } else {
                        reject(new Error(`ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ í˜•ì‹: ${JSON.stringify(jsonResponse)}`));
                    }
                } catch (error) {
                    reject(new Error(`ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: ${error.message}, ì‘ë‹µ: ${responseData.substring(0, 500)}`));
                }
            });
        });

        req.on('error', (error) => {
            reject(new Error(`Anthropic API ìš”ì²­ ì‹¤íŒ¨: ${error.message}`));
        });

        req.write(dataBuffer);
        req.end();
    });
}

module.exports = { callLLM };

